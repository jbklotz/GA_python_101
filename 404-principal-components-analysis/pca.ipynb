{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Prinicpal Components (Analysis)\n",
    "\n",
    "_Author:_ Timothy Book, General Assembly DC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "1. **What** problem(s) will PCA solve?\n",
    "1. **How** does it solve these problems?\n",
    "1. **When** are some times when PCA is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What** problem(s) does PCA solve?\n",
    "PCA is an **unsupervised learning** method. So, it is often used to **preprocess** data before it goes into a supervised learning method. Specifically, it can be used to \"solve\" the following common problems:\n",
    "\n",
    "1. Too many columns ($p \\gg n$)\n",
    "1. Multicollinearity\n",
    "1. Usually, both of the above at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How** does it work?\n",
    "Suppose you have $p$ feature columns. The **first principal component** is a linear combination of all $p$ columns that accounts for the **maximum variance** among them.  That is,\n",
    "\n",
    "$$z_1 = c_1x_1 + c_2x_2 + \\cdots + c_px_p$$\n",
    "\n",
    "The **second principal component** is another linear combination of the $p$ features that accounts for the maximum of the _remaining_ variance after the first. Another condition is that the second PC must be **orthogonal (perpendicular)** to the first.\n",
    "\n",
    "The **third principal component** maximizes the remaining variance while being orthogonal (read: _independent_) to the first two, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Geometric** interpretation:\n",
    "Suppose you have two $x$-variables that look like this:\n",
    "\n",
    "<img src=\"imgs/p1.png\" width=\"500px\"/>\n",
    "\n",
    "$x_1$ and $x_2$ are clearly correlated. So any $y$-variable we try to predict will have _multicollinearity_. So should we only pick one? How do we know which to pick? What if, instead of two $x$-variables, we have many? And their correlations are a little weaker? What do we do?!\n",
    "\n",
    "The geometric interpretation is the act of **rotating** axes to **decorrelate** your $x$-variables.\n",
    "\n",
    "<img src=\"imgs/p2.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **When** is this used IRL?\n",
    "In class, I frequently give the example of working with **genomic data**. It is expensive to gather rows, since genetic testing is difficult. But once you sample sommeone, you typically collect thousands of genetic markers (columns). However, only a few of them are significant, and many of them are correlated. Biostatisticians will often only use the first few PCs in their analyses.\n",
    "\n",
    "From a more data sciency perspective, PCA is often performed on image data to get low-resolution versions of images so they are easier to work with for other types of analyses. PCA is very heavily employed in **image processing** for this reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example 1 in Python**: Leggooooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bikeshare.csv', index_col='datetime', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = ['temp', 'atemp', 'humidity', 'holiday', 'workingday', 'windspeed']\n",
    "X = df[xvars]\n",
    "y = df['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 6):\n",
    "    sc = StandardScaler()\n",
    "    X_sc = sc.fit_transform(X)\n",
    "    pc = PCA(n_components=k)\n",
    "    X_pc = pc.fit_transform(X_sc)\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_pc, y)\n",
    "    print(lm.score(X_pc, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_sc = sc.fit_transform(X)\n",
    "pc = PCA(n_components=6)\n",
    "pc.fit(X_sc)\n",
    "plt.plot(range(1, 7), pc.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessing is now starting to get pretty complicated. Luckily, `sklearn` gives us a way to smash it all together in a data science \"pipeline.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('pc', PCA(n_components=2)),\n",
    "    ('lm', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)\n",
    "pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example 2**: Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating = pd.read_csv('data/speed_dating.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dating.iloc[:, 2:].corr(), cmap='coolwarm', vmin=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('pc', PCA()),\n",
    "    ('lm', LinearRegression())\n",
    "])\n",
    "\n",
    "X = dating.iloc[:, 2:].drop('objective_attractiveness', axis=1)\n",
    "y = dating['objective_attractiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsq_list = []\n",
    "for k in range(1, X.shape[1] + 1):\n",
    "    pipe.set_params(pc__n_components=k)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    rsq = pipe.score(X_test, y_test)\n",
    "    rsq_list.append(rsq)\n",
    "    print(f\"k = {k}: Rsq = {rsq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rsq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
